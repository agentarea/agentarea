"""Test real LLM tool calls with Ollama/qwen2.5 to identify malformed response source."""

import asyncio
import json
import logging
import os
from uuid import uuid4

import pytest

from agentarea_agents_sdk.models.llm_model import LLMModel, LLMRequest

logger = logging.getLogger(__name__)


@pytest.mark.asyncio
async def test_ollama_qwen25_tool_calls_direct():
    """Test direct LLM calls with Ollama/qwen2.5 to see raw responses."""
    
    # Skip if no Ollama available
    docker_host = os.environ.get("LLM_DOCKER_HOST", "localhost")
    endpoint_url = f"http://{docker_host}:11434"
    
    logger.info(f"Testing Ollama at {endpoint_url}")
    
    # Create LLM model
    llm_model = LLMModel(
        provider_type="ollama_chat",
        model_name="qwen2.5",
        endpoint_url=endpoint_url
    )
    
    # Define tools (same as production)
    tools = [
        {
            "type": "function",
            "function": {
                "name": "task_complete",
                "description": "Mark task as completed when you have finished the task successfully. Call this when you're done.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "result": {
                            "type": "string",
                            "description": "Optional final result or summary of what was accomplished"
                        }
                    },
                    "required": []
                }
            }
        }
    ]
    
    # Test messages
    messages = [
        {
            "role": "system",
            "content": "You are a helpful AI assistant. When you complete a task, use the task_complete tool."
        },
        {
            "role": "user",
            "content": "Please complete this simple test task by calling the task_complete tool."
        }
    ]
    
    request = LLMRequest(
        messages=messages,
        tools=tools,
        temperature=0.1,
        max_tokens=100
    )
    
    logger.info("=== Testing Non-Streaming LLM Call ===")
    try:
        response = await llm_model.complete(request)
        
        logger.info(f"Non-streaming response:")
        logger.info(f"  Content: {response.content}")
        logger.info(f"  Tool calls: {response.tool_calls}")
        logger.info(f"  Role: {response.role}")
        logger.info(f"  Cost: {response.cost}")
        
        if response.tool_calls:
            for i, tool_call in enumerate(response.tool_calls):
                logger.info(f"  Tool call {i}: {tool_call}")
        else:
            logger.warning("No tool calls in non-streaming response!")
            
    except Exception as e:
        logger.error(f"Non-streaming call failed: {e}")
        pytest.skip(f"Ollama not available: {e}")
    
    logger.info("\n=== Testing Streaming LLM Call ===")
    try:
        # Test streaming
        complete_content = ""
        complete_tool_calls = None
        chunk_count = 0
        
        async for chunk in llm_model.ainvoke_stream(request):
            chunk_count += 1
            logger.info(f"Chunk {chunk_count}:")
            logger.info(f"  Content: '{chunk.content}'")
            logger.info(f"  Tool calls: {chunk.tool_calls}")
            logger.info(f"  Role: {chunk.role}")
            
            if chunk.content:
                complete_content += chunk.content
            
            if chunk.tool_calls:
                complete_tool_calls = chunk.tool_calls
        
        logger.info(f"\nStreaming summary:")
        logger.info(f"  Total chunks: {chunk_count}")
        logger.info(f"  Complete content: '{complete_content}'")
        logger.info(f"  Final tool calls: {complete_tool_calls}")
        
        # Check if we have the malformed response issue
        if not complete_tool_calls and complete_content:
            logger.warning("ðŸš¨ MALFORMED RESPONSE DETECTED!")
            logger.warning(f"Tool calls are None but content contains: {complete_content}")
            
            # Try to parse content as JSON
            try:
                parsed = json.loads(complete_content.strip())
                if isinstance(parsed, dict) and "name" in parsed:
                    logger.warning(f"Content appears to be a tool call: {parsed}")
            except json.JSONDecodeError:
                logger.info("Content is not valid JSON")
        
    except Exception as e:
        logger.error(f"Streaming call failed: {e}")


@pytest.mark.asyncio
async def test_ollama_qwen25_various_prompts():
    """Test various prompts to see when malformed responses occur."""
    
    docker_host = os.environ.get("LLM_DOCKER_HOST", "localhost")
    endpoint_url = f"http://{docker_host}:11434"
    
    llm_model = LLMModel(
        provider_type="ollama_chat",
        model_name="qwen2.5",
        endpoint_url=endpoint_url
    )
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "task_complete",
                "description": "Mark task as completed",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "result": {"type": "string", "description": "Result"}
                    },
                    "required": []
                }
            }
        }
    ]
    
    test_cases = [
        {
            "name": "Simple completion request",
            "messages": [
                {"role": "user", "content": "Complete this task using the task_complete tool."}
            ]
        },
        {
            "name": "Vague task (like production)",
            "messages": [
                {"role": "user", "content": "test"}
            ]
        },
        {
            "name": "Explicit tool instruction",
            "messages": [
                {"role": "system", "content": "You must use the task_complete tool when done."},
                {"role": "user", "content": "Do a simple task and complete it."}
            ]
        }
    ]
    
    for test_case in test_cases:
        logger.info(f"\n=== Testing: {test_case['name']} ===")
        
        request = LLMRequest(
            messages=test_case["messages"],
            tools=tools,
            temperature=0.1,
            max_tokens=150
        )
        
        try:
            # Test streaming response
            complete_content = ""
            complete_tool_calls = None
            
            async for chunk in llm_model.ainvoke_stream(request):
                if chunk.content:
                    complete_content += chunk.content
                if chunk.tool_calls:
                    complete_tool_calls = chunk.tool_calls
            
            logger.info(f"Result:")
            logger.info(f"  Content: '{complete_content}'")
            logger.info(f"  Tool calls: {complete_tool_calls}")
            
            # Analyze the response
            if not complete_tool_calls and complete_content:
                # Check if content looks like a tool call
                if "task_complete" in complete_content.lower():
                    logger.warning(f"ðŸš¨ MALFORMED: Tool call in content, not tool_calls field")
                    
                    # Try to extract JSON
                    try:
                        # Look for JSON patterns
                        import re
                        json_pattern = r'\{[^}]*"name"[^}]*\}'
                        matches = re.findall(json_pattern, complete_content)
                        if matches:
                            logger.warning(f"Found JSON patterns: {matches}")
                    except Exception as e:
                        logger.debug(f"JSON extraction failed: {e}")
            
        except Exception as e:
            logger.error(f"Test case '{test_case['name']}' failed: {e}")


@pytest.mark.asyncio
async def test_litellm_direct_ollama():
    """Test litellm directly to see if the issue is in our wrapper or litellm itself."""
    
    import litellm
    
    docker_host = os.environ.get("LLM_DOCKER_HOST", "localhost")
    base_url = f"http://{docker_host}:11434"
    
    logger.info(f"=== Testing litellm directly with Ollama at {base_url} ===")
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "task_complete",
                "description": "Mark task as completed",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "result": {"type": "string", "description": "Result"}
                    },
                    "required": []
                }
            }
        }
    ]
    
    messages = [
        {"role": "user", "content": "Complete this task using the task_complete tool."}
    ]
    
    try:
        # Test non-streaming
        logger.info("Non-streaming litellm call:")
        response = await litellm.acompletion(
            model="ollama_chat/qwen2.5",
            messages=messages,
            tools=tools,
            base_url=base_url,
            temperature=0.1,
            max_tokens=100
        )
        
        message = response.choices[0].message
        logger.info(f"  Content: '{message.content}'")
        logger.info(f"  Tool calls: {getattr(message, 'tool_calls', None)}")
        
        # Test streaming
        logger.info("\nStreaming litellm call:")
        response_stream = await litellm.acompletion(
            model="ollama_chat/qwen2.5",
            messages=messages,
            tools=tools,
            base_url=base_url,
            temperature=0.1,
            max_tokens=100,
            stream=True
        )
        
        complete_content = ""
        tool_calls_seen = False
        chunk_count = 0
        
        async for chunk in response_stream:
            chunk_count += 1
            if chunk.choices:
                delta = chunk.choices[0].delta
                
                logger.info(f"  Chunk {chunk_count}:")
                logger.info(f"    Content: '{getattr(delta, 'content', '')}'")
                logger.info(f"    Tool calls: {getattr(delta, 'tool_calls', None)}")
                
                if hasattr(delta, 'content') and delta.content:
                    complete_content += delta.content
                
                if hasattr(delta, 'tool_calls') and delta.tool_calls:
                    tool_calls_seen = True
                    logger.info(f"    Tool call details: {delta.tool_calls}")
        
        logger.info(f"\nStreaming summary:")
        logger.info(f"  Complete content: '{complete_content}'")
        logger.info(f"  Tool calls seen: {tool_calls_seen}")
        
        if not tool_calls_seen and "task_complete" in complete_content.lower():
            logger.warning("ðŸš¨ LITELLM ISSUE: Tool call in content, not in tool_calls!")
            
    except Exception as e:
        logger.error(f"litellm direct test failed: {e}")
        pytest.skip(f"Ollama not available: {e}")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    pytest.main([__file__, "-v", "-s"])